import re
import conllu

# the conllu library parses the file into sentences and each sentence into tokens; each token is a dictionary with with the following keys:
# id, form, lemma, upos, xpos, feats, head, deprel, deps, misc.

simple_equivalents = ['ccomp', 'nsubj', 'nsubj:pass', 'csubj', 'csubj:pass', 'nummod', 'nmod:poss', 'xcomp', 'obl', 'amod', 'nmod', 'iobj', 'det:poss', 'obj', 'obl:arg', 'advmod', 'case', 'root', 'expl:pv']

def is_simp_equiv(deprel):
    if deprel in simple_equivalents:
        return True

    return False

complex_equivalents = {
    'auxilliaries': ['aux:pass', 'aux'],
    'determiners': ['det'],
    'compounds': ['flat', 'flat:name', 'compound', 'compound:prt'],
    'clausal_modifiers': ['appos', 'advcl', 'acl'],
    'complex_preds': ['expl', 'cop']
}

def is_comp_equiv(deprel):
    for key in complex_equivalents:
        if deprel in complex_equivalents[key]:
            return True

    return False

no_equivalents = {
    'sentence': ['reparandum', 'orphan', 'vocative'],
    'token': ['punct', 'discourse', 'mark']
}

def is_not_equiv(deprel):
    for key in no_equivalents:
        if deprel in no_equivalents[key]:
            return True
    
    return False

coordinants = ['conj', 'cc', 'parataxis']

def is_coordinant(deprel):
    if deprel in coordinants:
        return True

    return False

def is_deprel(deprel):
    if is_simp_equiv(deprel) == True or is_comp_equiv(deprel) == True or is_not_equiv(deprel) == True or is_coordinant(deprel) == True:
        return True
    
    return True

# every subset of deprels and their subsets are assigned and functions are defined to return True or False values for if a deprel is in a given subset.

generate_feat_gfs = input('automatically generate nonargument GFs from UD annotation?\ny/n: ')

class Token:
    def __init__(self, token):
        self.id = None
        self.id = token['id']

        self.form = None
        self.form = token['form']

        self.lemma = None
        self.lemma = token['lemma']

        self.upos = None
        self.upos = token['upos']

        self.feats = None

        if token['feats'] != '_':
            self.feats = token['feats']

        self.head = None
        self.head = token['head']

        self.deprel = None
        self.deprel = token['deprel']

        # most information in the UD annotation is stored in token objects and will be referred to in complex conversions and elsewhere.

        self.subtype = None

        if is_simp_equiv(self.deprel) == True:
            self.subtype = 'simple'

        elif is_comp_equiv(self.deprel) == True:
            if self.deprel in complex_equivalents['auxilliaries']:
                self.subtype = 'auxilliary'

            elif self.deprel in complex_equivalents['determiners']:
                self.subtype = 'determiner'

            elif self.deprel in complex_equivalents['compounds']:
                self.subtype = 'compound'

            elif self.deprel in complex_equivalents['clausal_modifiers']:
                self.subtype = 'clausal_modifier'

            elif self.deprel in complex_equivalents['complex_preds']:
                self.subtype = 'complex_pred'

        elif is_coordinant(self.deprel) == True:
            self.subtype = 'coordinant'

        else:
            Exception('TOKEN HAS NO VALID SUBTYPE :/')

        # the token is given a subtype which will be used to call an appropriate conversion method.

        self.gf = None
        self.value = None
        self.arg = None

        # the grammatical function, value and argument properties are generated by the conversion methods below.

        self.dependants = []
        self.arguments = []

        # the dependants and arguments lists are appended during the conversion (see. nest_order() and f_compose(sentence)) and required by pred_format().

    def convert_simple(self):
        if self.deprel == 'root':
            self.arg = True

            return 'ROOT', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'nsubj':
            self.arg = True

            return 'SUBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'nsubj:pass':
            self.arg = True

            return 'SUBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'csubj':
            self.arg = True

            return 'SUBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'csubj:pass':
            self.arg = True

            return 'SUBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'obj':
            self.arg = True

            return 'OBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'expl:pv':
            self.arg = True

            return 'OBJ', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'iobj':
            self.arg = True

            return 'OBJ:IND', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'obl':
            self.arg = True

            return 'OBL', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'obl:arg':
            self.arg = True

            return 'OBL', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'ccomp':
            self.arg = True

            return 'COMP', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'xcomp':
            self.arg = True

            return 'XCOMP', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'nmod':
            self.arg = False

            return 'ADJ', [{'PRED': '{}< >'.format(self.lemma)}]

        elif self.deprel == 'nmod:poss':
            self.arg = False

            return 'POSS', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'nummod':
            self.arg = False

            return 'SPEC', {'PRED': '{}< >'.format(self.lemma)}

        elif self.deprel == 'amod':
            self.arg = False

            return 'ADJ', [{'PRED': '{}< >'.format(self.lemma)}]
        
        elif self.deprel == 'advmod':
            self.arg = False

            return 'ADJ', [{'PRED': '{}< >'.format(self.lemma)}]

        elif self.deprel == 'det:poss':
            self.arg = False

            return 'POSS', {'PRED': '{}< >'.format(self.lemma)}
        
        elif self.deprel == 'case':
            self.arg = False

            if 'Case' in self.feats.keys():
                return 'CASE', {'PRED': '{}< >'.format(self.feats['Case']).upper()}

            else:
                return 'CASE', {'PRED': '{}'.format(self.lemma)}

        else:
            return 'NONE', {'NONE': 'NONE< >'}

    def convert_auxilliaries(self):
        self.arg = False

        return '*CPOUND', {'PRED': '{}-'.format(self.lemma)}

    def convert_determiners(self):
        definite_articles = ['der', 'die', 'das', 'des', 'den', 'dem']
        indefinite_articles = ['ein', 'eine', 'eines', 'einen', 'einem']

        self.arg = False

        if self.upos == 'DET':
            if self.lemma.lower() in definite_articles:
                return 'DEF', {'PRED': '+'}

            elif self.lemma.lower() in indefinite_articles:
                return 'DEF', {'PRED': '-'}

            else:
                return 'SPEC', {'PRED': '{}< >'.format(self.lemma)}
            
        elif self.upos == 'PRON':
            return 'POSS', {'PRED': '{}< >'.format(self.lemma)}

        else:
            print('unexpected determiner in the bagging area')

    def convert_compounds(self):
        self.arg = False

        if self.deprel == 'compound:prt':
            return '*CPOUND', {'PRED': '{}'.format(self.lemma)}

        else:
            return '*CPOUND', {'PRED': '-{}'.format(self.lemma)}

        # the dummy gf '*CPOUND' will not be part of the f_structure, it's just a marker.

    # def convert_claus_mods(self):

    # def convert_comp_preds(self):

    def convert_coordinants(self):
        if self.deprel == 'conj':
            self.arg = False

            return '*COORD', [{'PRED': '{}< >'.format(self.lemma), 'COORD': 'LIST'}]

            # conjunts have the coordination type LIST, because if this type is not updated by a dependant with the relation 'cc' (below), it probably is.
            # the dummy gf '*COORD' will not be part of the f_structure, it's just a marker.

        elif self.deprel == 'parataxis':
            self.arg = False

            return '*COORD', [{'PRED': '{}< >'.format(self.lemma), 'COORD': 'PARATAXIS'}]

        elif self.deprel == 'cc':
            self.arg = False

            return 'COORD', {'PRED': '{}'.format(self.lemma).upper()}

            # the only purpose of the value of coordinating words like is to update the coordination type of the conjunct.

        else:
            print('something\'s wrong with coordination...')

    def generate_feat_gfs(self):
        feat_gf_upos = ['NOUN', 'VERB', 'PRON', 'PROPN']

        if self.upos in feat_gf_upos:
            if 'Case' in self.feats.keys():
                try:
                    self.value['CASE'] = {'PRED': self.feats['Case'].upper()}

                except:
                    self.value[0]['CASE'] = {'PRED': self.feats['Case'].upper()}
            
            if 'Gender' in self.feats.keys():
                try:
                    self.value['GEN'] = {'PRED': self.feats['Gender'].upper()}

                except:
                    self.value[0]['GEN'] = {'PRED': self.feats['Gender'].upper()}
            
            if 'Number' in self.feats.keys():
                try:
                    self.value['NUM'] = {'PRED': self.feats['Number'].upper()}

                except:
                    self.value[0]['NUM'] = {'PRED': self.feats['Number'].upper()}
            
            if 'Person' in self.feats.keys():
                try:
                    self.value['PERS'] = {'PRED': self.feats['Person'].upper()}

                except:
                    self.value[0]['PERS'] = {'PRED': self.feats['Person'].upper()}
            
            if 'Mood' in self.feats.keys():
                try:
                    self.value['MOOD'] = {'PRED': self.feats['Mood'].upper()}

                except:
                    self.value[0]['MOOD'] = {'PRED': self.feats['Mood'].upper()}

            if 'Person' in self.feats.keys():
                try:
                    self.value['PERS'] = {'PRED': self.feats['Person'].upper()}

                except:
                    self.value[0]['PERS'] = {'PRED': self.feats['Person'].upper()}

            if 'Tense' in self.feats.keys():
                try:
                    self.value['TENSE'] = {'PRED': self.feats['Tense'].upper()}

                except:
                    self.value[0]['TENSE'] = {'PRED': self.feats['Tense'].upper()}

            if 'Aspect' in self.feats.keys():
                try:
                    self.value['ASP'] = {'PRED': self.feats['Aspect'].upper()}

                except:
                    self.value[0]['ASP'] = {'PRED': self.feats['Aspect'].upper()}

    def convert(self):
        if self.subtype == 'simple':
            self.gf, self.value = self.convert_simple()

        elif self.subtype == 'auxilliary':
            self.gf, self.value = self.convert_auxilliaries()

        elif self.subtype == 'determiner':
            self.gf, self.value = self.convert_determiners()

        elif self.subtype == 'compound':
            self.gf, self.value = self.convert_compounds()

        # elif self.subtype == 'clausal_modifier':
            # self.gf, self.value = self.convert_claus_mods()

        # elif self.subtype == 'complex_pred':
            # self.gf, self.value = self.convert_comp_preds()
        
        elif self.subtype == 'coordinant':
            self.gf, self.value = self.convert_coordinants()

        else:
            self.gf, self.value = self.convert_simple()

        if generate_feat_gfs.lower() == 'y':
            self.generate_feat_gfs()

        # self.convert() returns two values: a key and a value for the f_structure dictionary of this token.

# in sum, the Token class stores data from the filtered sentences and has methods for conversion to grammatical functions.

def parse_filter(sentence):
    filtered_sentence = []
    
    for token in sentence:
        if is_deprel(token['deprel']) == False:
            return False
        
            # if the sentence has a token in it with a deprel value which is not in the set of accepted deprels, it is not parsable.

        elif token['deprel'] in no_equivalents['sentence']:
            return False

            # if the sentence has a token in it with a deprel value in the list no_equivalents['sentence'], it is not parsable.
            # deprels in this set are part of UD but deemed unparsable by the engineer.

        elif token['deprel'] in no_equivalents['token']:
            pass

            # if a token has a deprel value in the list no_equivalents['token'], that token is ignored.

        else:
            filtered_sentence.append(token)

            # any token which passes these criteria is appended to the filtered sentence, which is ready to be converted to an f_structure.

    return filtered_sentence

def f_hierarchy(token):
    if token.gf == '*CONJ':
        return 7

    elif token.gf == 'ADJ':
        return 7

    elif token.gf == 'SUBJ':
        return 0

    elif token.gf == 'OBJ':
        return 1
    
    elif re.search(r'^OBJ:', token.gf) != None:
        return 2

    elif re.search(r'^OBL:?', token.gf) != None:
        return 3

    elif token.gf == 'COMP' or token.gf == 'XCOMP':
        return 4

    else:
        return 5

    # the dummy GF '*CONJ' is given special status as the very lowest in the function hierary,
    # to ensure that the coordination with this dependant happens after the rest of the token's GF's value has already been generated.

def nest_order(tokens):
    ordered_tokens = []
    ordered_tokens_waiting_room = []
    ordered_tokens_reception = []
    terminal_dependants = []

    for token in tokens:
        for other_token in tokens:
            if token.id == other_token.head:
                token.dependants.append(other_token)

                # the tokens' .dependants properties are appended.

        token.dependants.sort(key=f_hierarchy)
        
        # the tokens have their dependants sorted according to the functional hierarchy,
        # with a special place for coordinating conjunctions and parataxis at the end (see. f_hierarchy()).
    
    for token in tokens:
        if len(token.dependants) == 0:
            terminal_dependants.append(token)

            # the terminal dependants are listed.
    
    for token in terminal_dependants:
        pred_format(token)

        # the terminal dependants have their preds formatted.

    for token in tokens:
        if token.head == 0:
            matrix_pred = token
    
            # the matrix predicate is found.

    ordered_tokens.append(matrix_pred)

    for dependant in matrix_pred.dependants:
        ordered_tokens_waiting_room.append(dependant)

        # the matrix predicate is in the ordered list. it's dependants are in the waiting room.
    
    while len(ordered_tokens) < len(tokens):
        for token in ordered_tokens_waiting_room:
            for dependant in token.dependants:
                ordered_tokens_reception.append(dependant)

        ordered_tokens = ordered_tokens + ordered_tokens_waiting_room
        ordered_tokens_waiting_room = ordered_tokens_reception
        ordered_tokens_reception = []

        # the matrix predicate's dependants' dependants arrive at reception.
        # the dependants in the waiting room move out of the loop into the ordered_tokens list, and the dependants at reception take their place.
        # the new dependants in the waiting room have their dependants arrive at reception
        # and the process loops until the ordered_tokens list is as long as the original list.
    
    ordered_heads = []

    for token in ordered_tokens:
        if token not in terminal_dependants:
            ordered_heads.append(token)

    ordered_heads.reverse()

    # the ordered_tokens list has the terminal dependants removed and its order is reversed to reflect the nesting order for f_compose().
    # ordered_tokens is now a list of every token which is the head of another token, ordered from the heads of the terminal dependants to the matrix predicate.
    # this order ensures that no token can nest its dependants inside its value until they have had their turn to nest their dependants and so on.

    return ordered_heads

def pred_format(token):
    exception_preds = ['DEF', 'CASE', 'GEN', 'PERS', 'MOOD', 'TENSE', 'NUM', 'ASP', 'COORD', '*COORD', '*CPOUND']

    if token.upos == 'PRON' and token.gf != 'SPEC':
        token.value['PRED'] = 'PRO'

        return

        # if the token is a pronoun and not a specifier, its PRED value must be PRO.

    elif len(token.arguments) != 0 and token.gf not in exception_preds:
        try:
            token_pred = token.value['PRED']

        except:
            token_pred = token.value[0]['PRED']

        open_arg, close_arg = token_pred.split(' ')
        arg_string = ''

        # if the token has arguments and is not an ADJ, open_arg and close_arg are fragments of the original PRED value.
        # the exception is for formatting PRED values inside coordinations.

        for argument in token.arguments:
            arg_string = arg_string + '(' + argument + ')'

        try:
            token.value['PRED'] = open_arg + arg_string + close_arg
            
        except:
            token.value[0]['PRED'] = open_arg + arg_string + close_arg

            # the new PRED value is formatted to include the names of the argument grammatical functions and added.

    else:
        if token.gf not in exception_preds:
            try:
                token.value['PRED'] = token.lemma

            except:
                token.value[0]['PRED'] = token.lemma

        # if the token has no arguments its PRED value is equal to its lemma, unless it is a special PRED value for GFs like DEF or CASE or a pronoun.
    
    # it is presumed that all ADJ and *COORD PREDs will be formatted before they are coordinated, if that is required (see. f_compose());
    # this function is only designed to be called when the ADJ or *COORD has just one item in its list.

# in sum, these four functions are the toolkit for f_compose(), which returns f_structures for sentences in the HDT-UD.
# parse_filter() allows or disallows and trims the sentences for conversion, f_hierarchy orders GFs on the functional hierarchy,
# nest_order calculates the correct order of nesting for the GFs and pred_format edits the values of the PRED GFs to list the arguments of the function or to otherwise be simplified.

def f_compose(sentence):
    f_structure = {}
    tokens = []

    for token in sentence:
        token_object = Token(token)
        token_object.convert()
        tokens.append(token_object)

        # the sentence's tokens are cast as objects of the type above, converted and listed.

    tokens = nest_order(tokens)

    # the tokens list now contains every token which is the head of another token, with a list of their dependants as a property, ordered for composition.

    for token in tokens:
        if token.head == 0:
            matrix_pred = token

            # the matrix predicate is located.

        for dependant in token.dependants:
            key, value = dependant.gf, dependant.value

            if dependant.arg == True:
                token.arguments.append(key)

                # if the dependant token is an argument, its key is added to its head's list of arguments for pred_format().

            if dependant.gf == '*COORD':
                coordination_list = []

                if type(token.value) is dict:
                    coordination_list.append(token.value)
                        
                elif type(token.value) is list:
                    for sub_value in token.value:
                        coordination_list.append(sub_value)

                for sub_value in dependant.value:
                    coordination_list.append(sub_value)

                token.value = coordination_list
                
                # if the dependant is a coordinated conjuntion or parataxis,
                # the value(s) of the token must be placed inside a list with the coordinated conjunction or parataxis.

            elif dependant.gf == 'COORD':
                    token.value[0]['COORD'] = value
                    
                # if the dependant is a COORD, it must be the dependant of a coordinated conjunction and is nested appropriately.
                # this may result in replacing the default COORD function values 'PARATAXIS' and 'LIST' (see. Token.convert_coordinants()).
                
            elif dependant.gf == '*CPOUND':
                compound_token = dependant.value['PRED']

                if compound_token[0] == '-':
                    new_pred = token.lemma + compound_token

                    try:
                        token.value['PRED'] = '{}< >'.format(new_pred)
                        token.form = '{}'.format(new_pred)
                        token.lemma = '{}'.format(new_pred)
                        
                    except:
                        token.value[0]['PRED'] = '{}< >'.format(new_pred)
                        token.form = '{}'.format(new_pred)
                        token.lemma = '{}'.format(new_pred)

                else:
                    new_pred = compound_token + token.lemma

                    try:
                        token.value['PRED'] = '{}< >'.format(new_pred)
                        token.form = '{}'.format(new_pred)
                        token.lemma = '{}'.format(new_pred)
                        
                    except:
                        token.value[0]['PRED'] = '{}< >'.format(new_pred)
                        token.form = '{}'.format(new_pred)
                        token.lemma = '{}'.format(new_pred)

                # if the dependant is part of a compound, is is placed at either the beginning or the end of the token's PRED value string.

            elif dependant.gf == 'ADJ':
                try:
                    if 'ADJ' in token.value.keys():
                        for sub_value in value:
                            token.value['ADJ'].append(sub_value)

                    else:
                        try:
                            token.value[key] = value

                        except:
                            token.value[0][key] = value
                
                except:
                    if 'ADJ' in token.value[0].keys():
                        for sub_value in value:
                            token.value['ADJ'].append(sub_value)

                    else:
                        try:
                            token.value[key] = value

                        except:
                            token.value[0][key] = value

                # if the dependant's GF is an ADJ, a check is performed to see if an ADJ function is already nested inside the token's value.
                # if there is one, the new ADJ value(s) is (are) appended to the original ADJ's list. if there isn't, the dependant's key and value are nested inside the token's value.
            
            else:
                try:
                    token.value[key] = value

                except:
                    token.value[0][key] = value
                            
                # if the dependant is none of the above (if it is simple),
                # the dependant's key and value are nested inside the token's value (the exception is for nesting inside coordinated values as above).

        pred_format(token)

        # the pred value of the token is formatted to list its arguments if it has any, and to be simplified if it does not.

    key, value = matrix_pred.gf, matrix_pred.value
    f_structure[key] = value

    # every token's conversion to a GF is nested inside the value of its head, and the matrix predicate is nested inside the empty f_structure.
    # because of nest_order(), no GF can be left out of this composition.

    return f_structure

with open('test_compounds.conllu', 'r') as hdt_ud_1to10000A_102001to112000B:
    parse = conllu.parse(hdt_ud_1to10000A_102001to112000B.read())
    
    for sentence in parse:
        filtered_sentence = parse_filter(sentence)

        if filtered_sentence == False:
            pass

        # if the sentence is returned by parse_filter, it has no punctuation, particles or markers, and is ready to be converted into an f-structure.

        f_structure = f_compose(filtered_sentence)
        print(f_structure)